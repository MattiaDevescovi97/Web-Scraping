{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT ALL RELEVANT LIBRARIES\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import pandas as pd\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DRIVER SETUP\n",
    "\n",
    "service = Service(r\"C:\\Users\\MattiaDevescovi\\Desktop\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "# open up a specific web page\n",
    "driver.get(\"https://www.farmae.it/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the list of links of pages that needs to be scraped\n",
    "n = int(input(\"Enter a number: \"))\n",
    "number_of_pages = list(range(32, n+1))\n",
    "pagination = \"https://www.farmae.it/erboristeria.html?p=\"\n",
    "link_list = [pagination + str(page) for page in number_of_pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index_link, link in enumerate(link_list):\n",
    "    driver.get(link)\n",
    "    time.sleep(3)\n",
    "    WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, \"//a[@class='product-item-link']\")))\n",
    "\n",
    "    ## Create a list with all the product links\n",
    "    html_tag = driver.find_elements(By.XPATH, '//a[@class=\"product-item-link\"]')\n",
    "    html_prod_links = [l.get_attribute('outerHTML') for l in html_tag]\n",
    "    prod_links = []\n",
    "    for link in html_prod_links:\n",
    "        url = re.findall('href=\"(.*?)\"', link)\n",
    "        prod_links.append(url[0])\n",
    "        time.sleep(2)\n",
    "\n",
    "    time.sleep(30)\n",
    "\n",
    "    ## Start iterating on every product\n",
    "    for index_products, url_products in enumerate(prod_links):\n",
    "        urls_list = \"'\"+url_products+\"'\"\n",
    "        driver.get(url_products)\n",
    "\n",
    "        # ERROR 404\n",
    "        product_image_error_tags = driver.find_elements(By.XPATH, '//*[@id=\"maincontent\"]/div[2]/div/div[4]/div/div/div[1]/figure/a/img[2]')\n",
    "        if len(product_image_error_tags) > 0:\n",
    "            error_image = product_image_error_tags[0].get_attribute('outerHTML')\n",
    "            if 'banner-404-20220913-it.jpg' in error_image:\n",
    "                continue\n",
    "\n",
    "        WebDriverWait(driver, 20).until(EC.visibility_of_element_located((By.XPATH, \"//span[@class='base']\")))\n",
    "        time.sleep(4)\n",
    "\n",
    "        # creation of a variable last_index so that I can stack all the dataframes of each page\n",
    "        last_index = len(prod_links)\n",
    "        \n",
    "        # PRODUCT ID (MINSAN)\n",
    "        product_id_tag = driver.find_element(By.XPATH, \"//h3[@itemprop='sku']\")\n",
    "        product_id = product_id_tag.text\n",
    "\n",
    "        # PRODUCT NAME\n",
    "        product_name_tag = driver.find_element(By.XPATH, \"//h1[@class='page-title']\")\n",
    "        product_name = product_name_tag.text\n",
    "\n",
    "        # PRODUCT DESCRIPTION\n",
    "        product_description_tag = driver.find_element(By.XPATH, \"//div[@class='brand-description']\")\n",
    "        product_description = product_description_tag.text\n",
    "        product_description\n",
    "\n",
    "        # PRODUCT CATEGORIZATION\n",
    "        product_SC_tag = driver.find_elements(By.XPATH, \"//span[@class='category']\")\n",
    "        text_list = [product_SC_tag.text for product_SC_tag in product_SC_tag]\n",
    "        # create two columns containing the second and third elements of the list\n",
    "        Super_Collection = []\n",
    "        Collection = []\n",
    "        Section = []\n",
    "        try:\n",
    "         Super_Collection.append(text_list[0])\n",
    "        except IndexError:\n",
    "         Super_Collection.append('')\n",
    "        try:\n",
    "         Collection.append(text_list[1])\n",
    "        except IndexError:\n",
    "         Collection.append('')\n",
    "        try:\n",
    "         Section.append(text_list[2])\n",
    "        except IndexError:\n",
    "         Section.append('')\n",
    "\n",
    "        # PRODUCT IMAGE\n",
    "        product_image_tag = driver.find_element(By.TAG_NAME, \"img\")\n",
    "        html_image = product_image_tag.get_attribute('outerHTML')\n",
    "        match = re.search('src=\"(.*?)\"', html_image)\n",
    "        image_url = []\n",
    "        if match:\n",
    "            image_link = match.group(1)\n",
    "            image_url.append(image_link)\n",
    "            \n",
    "        ### STRUCTURE SUBCATEGORY INTO A DATAFRAME\n",
    "        if index_products == 0:\n",
    "            pharma_master_catalogue = pd.DataFrame(\n",
    "                {'external_ID': product_id,\n",
    "                'Super Collection': Super_Collection,\n",
    "                'Collection': Collection,\n",
    "                'Section': Section,\n",
    "                'name': product_name,\n",
    "                'description': product_description, \n",
    "                'image': image_url\n",
    "                })\n",
    "        if index_products != 0:\n",
    "            temp_pharma_master_catalogue = pd.DataFrame(\n",
    "                {'external_ID': product_id,\n",
    "                'Super Collection': Super_Collection,\n",
    "                'Collection': Collection,\n",
    "                'Section': Section,\n",
    "                'name': product_name,\n",
    "                'description': product_description, \n",
    "                'image': image_url\n",
    "                })\n",
    "            data = [pharma_master_catalogue, temp_pharma_master_catalogue]\n",
    "            pharma_master_catalogue = pd.concat(data)\n",
    "        if index_products == last_index - 1:\n",
    "            if index_link == 0:\n",
    "                partner_menu = pharma_master_catalogue\n",
    "            if index_link != 0:\n",
    "                data = [partner_menu, pharma_master_catalogue]\n",
    "                partner_menu = pd.concat(data)\n",
    "        \n",
    "        time.sleep(4)\n",
    "    \n",
    "    # Save the dataframe to csv after every iteration of link_list\n",
    "    filename = f\"menu_farmaè_pg_{index_link}.csv\"\n",
    "    path_csv = (r\"C:\\Users\\MattiaDevescovi\\Desktop\\Scraping farmaè\\\\\")\n",
    "    partner_menu.to_csv(path_csv + filename, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GlovoPy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d8b9aefaf11418a6d930fdd0620f5117a1973b4fb274235a9eb047f7ebf7027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
